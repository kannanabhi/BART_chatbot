{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Xl1iYXC-S2",
        "outputId": "77d73126-e706-4c3b-b05d-5a5bf23eaeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Mini:   asafaya/bert-mini-arabic\n",
        "# Medium: asafaya/bert-medium-arabic\n",
        "# Base:   asafaya/bert-base-arabic\n",
        "# Large:  asafaya/bert-large-arabic\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
        "model = AutoModel.from_pretrained(\"asafaya/bert-base-arabic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEvONoPwHnOE",
        "outputId": "e67036ff-46f7-42d2-a1d5-fef33a238a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input=tokenizer(\"hello world\",return_tensors=\"pt\")\n",
        "output=model(**input)\n",
        "output"
      ],
      "metadata": {
        "id": "7_eZhzoJIio2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ad3eb0-1fab-4bfa-f4fd-9918c9ff5ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.2798, -1.1629, -0.0058,  ...,  0.1825, -1.1978,  0.8594],\n",
              "         [ 0.7338, -0.4763, -0.6216,  ...,  1.6464, -0.0477, -0.3953],\n",
              "         [ 0.5909, -1.3253, -0.6442,  ...,  1.6292, -0.3645, -0.0353],\n",
              "         [ 0.5445, -0.9365, -0.5767,  ...,  2.0991, -0.5011, -0.5042],\n",
              "         [ 0.2636, -0.6326, -0.6087,  ...,  1.1745, -0.5661, -0.3325]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 3.2557e-01,  9.0426e-02, -5.9612e-01,  3.0199e-01,  1.6431e-01,\n",
              "          3.0421e-01,  7.9883e-02,  1.6186e-01,  9.9588e-01,  5.9706e-01,\n",
              "          2.3691e-01,  3.8624e-01, -1.9090e-01, -1.8560e-02,  1.9613e-02,\n",
              "          3.3118e-02, -1.9063e-01, -1.8373e-01,  1.9237e-01,  2.5258e-01,\n",
              "          9.9885e-01,  1.5907e-01, -1.0823e-02,  1.4228e-02,  1.8242e-01,\n",
              "          2.7796e-01,  5.8006e-02,  1.7201e-01, -2.4368e-01,  9.8189e-01,\n",
              "         -3.5430e-01, -3.7075e-01, -2.9251e-01, -8.8830e-02,  4.2242e-01,\n",
              "         -8.2653e-02, -7.5886e-02, -6.9154e-02, -2.7366e-01, -9.9890e-01,\n",
              "          2.1575e-03, -2.7424e-01, -5.7307e-02,  1.3486e-01, -4.4009e-02,\n",
              "         -1.6364e-01, -2.3476e-01, -5.0348e-01,  6.4206e-02,  1.0212e-01,\n",
              "          1.9035e-01,  3.5175e-01, -9.1522e-01,  6.2536e-01,  9.9665e-01,\n",
              "         -1.3362e-01, -3.6231e-01, -9.4803e-02, -3.5377e-02,  1.3464e-01,\n",
              "         -3.3058e-02, -5.4394e-01, -9.9063e-01, -3.8822e-01,  4.1098e-01,\n",
              "         -2.0553e-01,  9.9928e-01, -3.1560e-01,  1.2225e-01, -2.7229e-01,\n",
              "         -3.2027e-01, -6.9274e-02, -8.2662e-01,  2.4959e-01, -1.9717e-01,\n",
              "         -2.0434e-01,  2.8661e-01,  6.7347e-01,  4.5078e-01,  2.3077e-01,\n",
              "          9.9358e-01,  4.6287e-01,  3.3791e-02,  9.9976e-01,  5.9634e-01,\n",
              "         -8.8601e-01,  2.3919e-01,  8.0548e-02, -7.1219e-01,  9.9871e-01,\n",
              "          9.6275e-01, -5.9998e-02, -3.0173e-01, -2.7064e-01, -1.2211e-01,\n",
              "          1.7478e-01, -9.9049e-01,  1.3081e-01, -4.0988e-01,  4.1075e-01,\n",
              "          3.8886e-01, -2.0121e-01, -9.6506e-02,  9.8780e-01, -3.6342e-01,\n",
              "          2.5233e-01, -9.9964e-01, -4.2546e-02,  9.1019e-01,  9.2450e-01,\n",
              "          9.4717e-01,  1.9485e-02,  2.2794e-01, -9.7679e-01,  7.5699e-02,\n",
              "          3.1724e-01,  8.5289e-01,  9.9950e-01,  9.9985e-01, -5.8422e-02,\n",
              "         -1.8792e-01, -1.8794e-01, -9.4204e-01, -5.8160e-01, -2.5764e-01,\n",
              "          1.8451e-01,  9.9378e-01,  9.9387e-01,  2.0233e-01, -1.9520e-01,\n",
              "         -6.7957e-03, -3.4218e-01, -2.3890e-01,  3.4606e-01, -5.7978e-02,\n",
              "          9.9854e-01,  6.5840e-01, -9.7380e-01,  1.1850e-01, -1.3378e-01,\n",
              "          1.0950e-01,  6.9457e-01, -4.0807e-02,  9.8658e-01,  9.3581e-02,\n",
              "          4.6201e-02, -9.5339e-01,  1.8480e-01,  9.9022e-01,  2.9891e-01,\n",
              "          2.2693e-01, -5.2048e-02, -9.8805e-01, -2.6431e-01,  9.5340e-03,\n",
              "         -4.4185e-01, -5.2870e-01,  1.1448e-01,  5.1392e-01,  9.7028e-01,\n",
              "         -2.9705e-01,  7.0301e-01, -9.6470e-01, -1.6248e-01,  3.0649e-01,\n",
              "          9.9876e-01,  2.7707e-01, -5.1145e-01, -1.0102e-01, -6.0353e-01,\n",
              "         -2.3395e-03,  9.3204e-01, -9.6661e-02, -9.9716e-01, -4.2509e-01,\n",
              "          9.4903e-01, -2.2124e-01,  2.0857e-01,  3.4469e-01,  1.7327e-01,\n",
              "         -9.7147e-01,  9.9136e-02, -1.9188e-01, -4.2931e-01,  4.5647e-01,\n",
              "          9.7891e-01,  4.8435e-01, -1.2918e-01, -3.7744e-01, -6.8371e-02,\n",
              "          3.2847e-01, -1.4499e-01, -2.1481e-01,  5.3998e-01,  2.1653e-01,\n",
              "          8.4142e-01, -5.8894e-01, -2.9297e-02,  1.1206e-01,  5.2365e-01,\n",
              "          8.1834e-02,  4.6324e-01, -5.4006e-01,  4.2664e-01, -9.7016e-01,\n",
              "          8.0798e-02, -1.4000e-01,  4.0292e-01,  7.0209e-02, -5.9386e-01,\n",
              "          2.2221e-01, -3.9605e-01,  9.9102e-01,  9.6706e-01,  1.2979e-01,\n",
              "         -9.8419e-01,  1.5206e-01,  9.9404e-01,  1.6808e-01,  3.4014e-01,\n",
              "         -1.3588e-01, -8.4575e-03,  9.3454e-01,  4.9929e-01, -6.5953e-03,\n",
              "         -7.5786e-01,  9.2380e-01, -9.9411e-01, -1.4491e-03,  4.3276e-01,\n",
              "          9.9751e-01, -3.8170e-02,  1.8135e-01,  3.9781e-01, -6.4408e-01,\n",
              "          2.4272e-01, -9.8579e-01, -4.7908e-01,  4.9677e-01,  2.2267e-01,\n",
              "          9.9923e-01, -3.3668e-01, -9.4421e-01, -9.9443e-01, -2.5060e-01,\n",
              "         -3.6619e-01,  2.9881e-01, -7.1060e-02, -9.9768e-01,  9.8864e-01,\n",
              "          5.1576e-01,  2.4955e-02,  2.8820e-01, -3.8472e-01, -8.9019e-02,\n",
              "         -1.7196e-01, -3.4260e-01,  9.5533e-01,  8.9961e-02,  1.0407e-03,\n",
              "          4.8112e-01,  7.4747e-01,  9.9930e-01,  9.9403e-01, -2.5616e-01,\n",
              "         -4.2045e-01, -2.7013e-01, -6.6268e-02, -9.9765e-01, -7.6076e-01,\n",
              "         -4.9005e-02, -3.6625e-01,  3.9265e-01,  3.1385e-01,  1.5444e-02,\n",
              "          9.9837e-01,  2.4201e-01,  9.9665e-01,  9.9853e-01, -2.9551e-01,\n",
              "         -9.9996e-01, -2.8844e-01,  9.9660e-01,  1.8868e-01,  3.3116e-01,\n",
              "         -4.0981e-01,  3.9331e-01, -9.9937e-01,  1.9553e-01, -9.9943e-01,\n",
              "         -1.4114e-01, -3.8353e-01,  3.6401e-02,  1.7587e-02,  9.9984e-01,\n",
              "         -1.7502e-01,  2.9429e-01,  4.8205e-01,  2.2078e-01,  1.4157e-01,\n",
              "          9.9994e-01, -2.2525e-01,  3.5686e-01,  7.2495e-01,  9.9704e-01,\n",
              "         -4.8141e-02,  6.2644e-03,  9.7785e-01, -1.6259e-01, -6.3722e-01,\n",
              "          1.3254e-01, -3.0111e-01, -4.4501e-01,  2.0894e-01, -9.9941e-01,\n",
              "          5.5728e-01, -1.4685e-01, -3.2108e-01, -2.8751e-01, -8.9346e-01,\n",
              "         -4.9715e-01, -8.5610e-01,  1.8749e-01,  3.9360e-02,  2.9015e-01,\n",
              "          4.2924e-01,  2.8254e-01,  7.5485e-01, -9.9963e-01,  9.9956e-01,\n",
              "          5.2695e-01, -1.8460e-01, -9.8821e-01, -1.6851e-01, -2.3734e-01,\n",
              "         -2.2081e-02,  4.8332e-02, -2.5381e-02, -1.6258e-01,  9.3860e-01,\n",
              "          3.1098e-02, -7.8576e-02, -9.9941e-01, -3.1950e-01, -9.3551e-01,\n",
              "         -2.1546e-01,  2.5213e-01,  5.5669e-02, -2.3690e-01, -9.9988e-01,\n",
              "         -5.9934e-02, -3.9610e-01, -3.2907e-01,  4.0110e-01,  4.9852e-01,\n",
              "          5.9695e-01,  2.4512e-01,  4.5454e-01, -7.9206e-01, -9.8168e-01,\n",
              "         -6.5527e-01, -3.3238e-01, -7.6276e-01,  9.3163e-01,  2.3153e-01,\n",
              "          5.2215e-02, -1.1949e-01,  3.2535e-01, -1.2713e-01, -4.2837e-01,\n",
              "         -9.8541e-01,  1.7684e-02, -3.6538e-01,  3.1459e-01, -8.3673e-01,\n",
              "         -9.3400e-01,  9.7399e-01,  1.2932e-01,  1.1744e-01,  5.7172e-01,\n",
              "         -9.9652e-01,  6.1376e-02,  4.5061e-02,  7.3968e-01, -1.9303e-01,\n",
              "         -6.1997e-02, -1.6587e-01, -1.7104e-01,  4.8733e-01,  1.6183e-01,\n",
              "         -9.3801e-01,  5.2966e-02, -2.3940e-01, -4.4850e-01, -9.9661e-01,\n",
              "         -9.7644e-01, -1.3221e-01,  9.5831e-01,  2.8630e-01,  5.2371e-01,\n",
              "         -9.9905e-01, -3.2412e-01,  1.1662e-01,  5.0411e-01,  8.1118e-01,\n",
              "          5.8735e-01,  7.9779e-01,  2.4330e-01, -9.7111e-01,  2.5692e-01,\n",
              "         -3.6096e-01,  2.4915e-01, -9.9010e-01, -9.9880e-01,  2.8914e-02,\n",
              "          5.0831e-01, -3.6342e-01,  1.7886e-01, -1.4505e-01, -5.5372e-02,\n",
              "          5.6323e-01, -8.5945e-03, -2.2506e-01,  5.0021e-01,  1.8589e-01,\n",
              "         -2.9299e-01,  4.5481e-01,  9.9968e-01,  9.4702e-02,  9.9937e-01,\n",
              "         -9.6339e-01,  5.4495e-01,  2.3836e-01, -3.5234e-01,  7.5643e-01,\n",
              "         -9.9835e-01, -9.5694e-02,  1.4401e-01,  9.9847e-01,  6.2433e-01,\n",
              "          9.9783e-01,  2.0633e-01,  8.3420e-02, -4.1050e-01,  3.0892e-01,\n",
              "          9.9676e-01,  2.2759e-01,  1.1786e-01,  4.9254e-01,  7.7106e-01,\n",
              "          1.4876e-01,  3.4727e-01,  6.6551e-02, -5.8013e-03, -1.5741e-01,\n",
              "          5.5050e-02,  9.9782e-01,  2.2402e-01,  2.9427e-02, -9.9885e-01,\n",
              "          4.6745e-02, -2.9405e-02, -4.3680e-01, -9.9959e-01,  9.4322e-01,\n",
              "         -2.3758e-01,  2.9307e-01,  8.7491e-01, -3.6138e-01,  2.9008e-01,\n",
              "         -3.9170e-01, -7.5755e-01, -2.5012e-02, -7.0944e-02,  3.3367e-01,\n",
              "          8.1538e-01,  1.9527e-01, -1.0362e-01, -2.0339e-01,  9.0927e-01,\n",
              "         -3.9815e-01, -2.1089e-01, -9.9945e-01,  5.8487e-01, -1.2647e-01,\n",
              "          5.1180e-01, -5.7804e-02,  6.8780e-01,  2.2567e-01, -2.1335e-01,\n",
              "         -2.4838e-01,  1.4447e-01,  4.5497e-01,  2.0694e-01, -1.6258e-01,\n",
              "          5.7610e-01,  1.0900e-01,  3.4458e-01,  2.6794e-01, -8.9563e-01,\n",
              "          4.1533e-01, -5.1989e-01, -1.4907e-02, -2.6191e-01,  1.1000e-01,\n",
              "          3.8129e-01,  2.8093e-01,  3.5066e-01, -1.5436e-01,  9.9765e-01,\n",
              "         -5.6755e-01, -4.7227e-02,  5.2341e-01,  2.1886e-01, -5.8754e-01,\n",
              "          5.5354e-01, -3.0372e-02,  2.2168e-03, -3.7655e-01,  9.9987e-01,\n",
              "         -1.1839e-01,  3.0354e-02,  6.8534e-02,  2.4128e-01,  9.8480e-01,\n",
              "         -3.6375e-01,  9.9947e-01,  4.9238e-01, -3.0674e-01,  3.4457e-02,\n",
              "          9.9729e-01, -1.7333e-01, -4.1333e-02, -6.0153e-02, -3.8144e-01,\n",
              "          5.0128e-01, -4.6334e-01, -1.0182e-01,  1.2473e-02, -1.5061e-02,\n",
              "          1.5693e-01, -3.0337e-01,  2.2070e-01,  9.7154e-01, -1.8947e-01,\n",
              "         -7.1769e-01, -1.5221e-01,  8.4129e-01, -1.2630e-01,  3.2177e-01,\n",
              "          5.4286e-02,  1.8145e-01, -1.3833e-01,  1.5854e-01, -3.5509e-01,\n",
              "         -1.6060e-01,  1.6446e-01,  1.4951e-01, -5.9990e-01,  9.2257e-01,\n",
              "          1.0640e-01,  3.4218e-01,  8.2349e-02,  1.6660e-01, -4.3423e-02,\n",
              "          9.9933e-01,  1.0544e-01,  2.8703e-02,  2.5063e-01, -9.9388e-01,\n",
              "          1.5022e-01,  2.8454e-01,  2.7309e-01,  9.9888e-01,  4.7212e-01,\n",
              "         -1.1229e-01, -9.9530e-01,  8.1064e-01, -5.1700e-01,  3.3779e-01,\n",
              "         -1.0266e-01,  2.7538e-01,  4.7976e-01, -3.0236e-01,  4.7524e-01,\n",
              "          1.3073e-01,  3.8210e-01, -1.2459e-01,  9.9808e-01,  9.9772e-01,\n",
              "         -9.9080e-01,  1.8181e-01,  9.9978e-01, -1.3953e-01,  2.3449e-01,\n",
              "          9.1776e-01, -7.4246e-01,  6.7598e-01, -6.8682e-01, -4.7693e-02,\n",
              "         -3.5446e-01, -4.9457e-01, -3.6252e-01, -7.6314e-01,  9.9949e-01,\n",
              "         -9.9994e-01, -6.6671e-01, -5.1947e-01,  7.0641e-02, -4.4458e-01,\n",
              "         -1.0791e-02, -2.5374e-01, -2.0077e-01,  9.4241e-01, -4.2390e-01,\n",
              "          9.9903e-01,  9.8536e-01, -2.2778e-04,  5.2392e-01,  3.3415e-01,\n",
              "         -9.9963e-01,  9.8096e-01, -6.0298e-01, -9.7993e-01, -1.3125e-01,\n",
              "          4.6200e-01,  4.1184e-01,  4.0768e-01, -4.7631e-01,  4.2452e-01,\n",
              "         -9.3704e-01,  2.3138e-01, -2.1780e-01, -9.9205e-01,  3.6274e-01,\n",
              "         -3.2567e-01,  9.9858e-01, -5.0112e-01,  6.1764e-03, -9.9166e-01,\n",
              "          9.2757e-01,  9.4513e-01,  3.9825e-01, -8.2287e-01,  3.9317e-02,\n",
              "          2.5298e-01,  6.2477e-01, -7.5590e-02,  3.6115e-01,  1.1481e-01,\n",
              "          1.2807e-01,  1.6726e-01, -9.4149e-01, -8.5660e-01,  2.6033e-03,\n",
              "         -4.6527e-02,  9.9036e-01,  5.1574e-01, -6.7826e-01,  1.7738e-01,\n",
              "          4.2490e-01, -2.9537e-02, -2.0266e-01, -1.1175e-02, -8.1920e-01,\n",
              "          5.4377e-01, -9.9828e-01, -9.6628e-01,  2.0522e-01, -2.5189e-01,\n",
              "          2.5341e-01, -9.9763e-03,  9.9912e-01,  2.0920e-01, -1.4289e-01,\n",
              "          4.2243e-01,  9.8949e-01,  1.5945e-01, -2.7217e-01, -4.3239e-01,\n",
              "          1.2770e-01,  4.5460e-01, -3.0965e-01,  9.9920e-01,  4.9653e-01,\n",
              "         -4.3619e-02, -1.6094e-01, -4.3450e-01,  6.5900e-01,  1.0718e-01,\n",
              "          9.7932e-02,  5.2086e-01,  9.2529e-01,  7.3000e-02,  1.8618e-01,\n",
              "          9.5842e-01, -9.6492e-01,  3.1261e-01,  6.9616e-03, -1.7974e-01,\n",
              "         -5.7595e-02,  1.2428e-01, -2.3371e-01, -1.3755e-01,  1.4305e-01,\n",
              "          9.7568e-01,  3.0617e-01,  2.2461e-01, -1.6621e-01, -4.9711e-02,\n",
              "          1.2564e-01, -2.9527e-01,  9.9607e-01,  9.9667e-01,  1.6623e-01,\n",
              "         -4.5997e-01, -3.8863e-02, -1.5446e-01, -1.6504e-01, -9.0133e-02,\n",
              "          9.8590e-01,  7.2079e-01, -4.0194e-01,  5.7046e-01,  2.3467e-01,\n",
              "          2.6859e-01,  5.1891e-02,  8.6669e-01,  9.9709e-01,  3.1085e-01,\n",
              "          4.6259e-01,  2.9604e-02, -5.3089e-01, -1.1166e-01,  9.9954e-01,\n",
              "         -5.8848e-01, -9.9618e-01, -5.8427e-02, -1.2707e-01,  3.4910e-01,\n",
              "          7.1518e-02, -5.9399e-01, -4.1677e-01, -2.4255e-02, -7.1969e-01,\n",
              "          4.5412e-01, -6.0505e-02, -9.7918e-01, -4.9807e-01, -9.8780e-01,\n",
              "         -3.0834e-02, -4.0291e-01, -4.0108e-01,  9.9020e-01, -5.5598e-02,\n",
              "         -9.8523e-01, -3.3847e-01, -9.9915e-01, -2.5124e-01,  5.3903e-01,\n",
              "         -9.9976e-01, -1.6499e-02, -9.9961e-01,  5.7992e-03,  2.3660e-01,\n",
              "          6.6860e-01,  9.6354e-01, -2.5474e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install preprocessing"
      ],
      "metadata": {
        "id": "PhCfM1ST1Uh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2657c1b2-b1e1-4f25-cb8f-586682d061eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting preprocessing\n",
            "  Downloading preprocessing-0.1.13-py3-none-any.whl (349 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.6/349.6 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-rtd-theme==0.2.4\n",
            "  Downloading sphinx_rtd_theme-0.2.4-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk==3.2.4\n",
            "  Downloading nltk-3.2.4.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from nltk==3.2.4->preprocessing) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.4-py3-none-any.whl size=1367720 sha256=3033d9059894b1e540eeb3d0a8f9ceb576c1487498e4359b3accf5a369707495\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/37/86/b5270b826e4b542bd6791005300c9d3864059901c7efc03545\n",
            "Successfully built nltk\n",
            "Installing collected packages: sphinx-rtd-theme, nltk, preprocessing\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed nltk-3.2.4 preprocessing-0.1.13 sphinx-rtd-theme-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From here we the qna bot code starts"
      ],
      "metadata": {
        "id": "Eqi9NuKF7PVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "basically the below is the entire code which I had copied from this following github link\n",
        "https://github.com/aub-mind/arabert/blob/master/preprocess.py"
      ],
      "metadata": {
        "id": "UvmQZXf77U6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarabic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMlmTLNG2Jp4",
        "outputId": "cce666d8-5432-4c9e-fb85-dc1c0f79b187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.8/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import html\n",
        "import logging\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "import pyarabic.araby as araby\n",
        "\n",
        "ACCEPTED_MODELS = [\n",
        "    \"bert-base-arabertv01\",\n",
        "    \"bert-base-arabert\",\n",
        "    \"bert-base-arabertv02\",\n",
        "    \"bert-base-arabertv2\",\n",
        "    \"bert-base-arabertv02-twitter\",\n",
        "    \"bert-large-arabertv02\",\n",
        "    \"bert-large-arabertv2\",\n",
        "    \"bert-large-arabertv02-twitter\",\n",
        "    \"araelectra-base\",\n",
        "    \"araelectra-base-discriminator\",\n",
        "    \"araelectra-base-generator\",\n",
        "    \"araelectra-base-artydiqa\",\n",
        "    \"aragpt2-base\",\n",
        "    \"aragpt2-medium\",\n",
        "    \"aragpt2-large\",\n",
        "    \"aragpt2-mega\",\n",
        "]\n",
        "\n",
        "SEGMENTED_MODELS = [\n",
        "    \"bert-base-arabert\",\n",
        "    \"bert-base-arabertv2\",\n",
        "    \"bert-large-arabertv2\",\n",
        "]\n",
        "\n",
        "SECOND_GEN_MODELS = [\n",
        "    \"bert-base-arabertv02\",\n",
        "    \"bert-base-arabertv2\",\n",
        "    \"bert-base-arabertv02-twitter\",\n",
        "    \"bert-large-arabertv02\",\n",
        "    \"bert-large-arabertv2\",\n",
        "    \"bert-large-arabertv02-twitter\",\n",
        "    \"araelectra-base\",\n",
        "    \"araelectra-base-discriminator\",\n",
        "    \"araelectra-base-generator\",\n",
        "    \"araelectra-base-artydiqa\",\n",
        "    \"aragpt2-base\",\n",
        "    \"aragpt2-medium\",\n",
        "    \"aragpt2-large\",\n",
        "    \"aragpt2-mega\",\n",
        "]\n",
        "\n",
        "TWEET_MODELS = [\n",
        "    \"bert-base-arabertv02-twitter\",\n",
        "    \"bert-large-arabertv02-twitter\",\n",
        "]\n",
        "\n",
        "PREFIX_LIST = [\n",
        "    \"ال\",\n",
        "    \"و\",\n",
        "    \"ف\",\n",
        "    \"ب\",\n",
        "    \"ك\",\n",
        "    \"ل\",\n",
        "    \"لل\",\n",
        "    \"\\u0627\\u0644\",\n",
        "    \"\\u0648\",\n",
        "    \"\\u0641\",\n",
        "    \"\\u0628\",\n",
        "    \"\\u0643\",\n",
        "    \"\\u0644\",\n",
        "    \"\\u0644\\u0644\",\n",
        "    \"س\",\n",
        "]\n",
        "SUFFIX_LIST = [\n",
        "    \"ه\",\n",
        "    \"ها\",\n",
        "    \"ك\",\n",
        "    \"ي\",\n",
        "    \"هما\",\n",
        "    \"كما\",\n",
        "    \"نا\",\n",
        "    \"كم\",\n",
        "    \"هم\",\n",
        "    \"هن\",\n",
        "    \"كن\",\n",
        "    \"ا\",\n",
        "    \"ان\",\n",
        "    \"ين\",\n",
        "    \"ون\",\n",
        "    \"وا\",\n",
        "    \"ات\",\n",
        "    \"ت\",\n",
        "    \"ن\",\n",
        "    \"ة\",\n",
        "    \"\\u0647\",\n",
        "    \"\\u0647\\u0627\",\n",
        "    \"\\u0643\",\n",
        "    \"\\u064a\",\n",
        "    \"\\u0647\\u0645\\u0627\",\n",
        "    \"\\u0643\\u0645\\u0627\",\n",
        "    \"\\u0646\\u0627\",\n",
        "    \"\\u0643\\u0645\",\n",
        "    \"\\u0647\\u0645\",\n",
        "    \"\\u0647\\u0646\",\n",
        "    \"\\u0643\\u0646\",\n",
        "    \"\\u0627\",\n",
        "    \"\\u0627\\u0646\",\n",
        "    \"\\u064a\\u0646\",\n",
        "    \"\\u0648\\u0646\",\n",
        "    \"\\u0648\\u0627\",\n",
        "    \"\\u0627\\u062a\",\n",
        "    \"\\u062a\",\n",
        "    \"\\u0646\",\n",
        "    \"\\u0629\",\n",
        "]\n",
        "\n",
        "\n",
        "# the never_split list is ussed with the transformers library\n",
        "_PREFIX_SYMBOLS = [x + \"+\" for x in PREFIX_LIST]\n",
        "_SUFFIX_SYMBOLS = [\"+\" + x for x in SUFFIX_LIST]\n",
        "_OTHER_TOKENS = [\"[رابط]\", \"[مستخدم]\", \"[بريد]\"]\n",
        "NEVER_SPLIT_TOKENS = list(set(_PREFIX_SYMBOLS + _SUFFIX_SYMBOLS + _OTHER_TOKENS))\n",
        "\n",
        "URL_REGEXES = [\n",
        "    r\"(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\",\n",
        "    r\"@(https?|ftp)://(-\\.)?([^\\s/?\\.#-]+\\.?)+(/[^\\s]*)?$@iS\",\n",
        "    r\"http[s]?://[a-zA-Z0-9_\\-./~\\?=%&]+\",\n",
        "    r\"www[a-zA-Z0-9_\\-?=%&/.~]+\",\n",
        "    r\"[a-zA-Z]+\\.com\",\n",
        "    r\"(?=http)[^\\s]+\",\n",
        "    r\"(?=www)[^\\s]+\",\n",
        "    r\"://\",\n",
        "]\n",
        "USER_MENTION_REGEX = r\"@[\\w\\d]+\"\n",
        "EMAIL_REGEXES = [r\"[\\w-]+@([\\w-]+\\.)+[\\w-]+\", r\"\\S+@\\S+\"]\n",
        "REDUNDANT_PUNCT_PATTERN = (\n",
        "    r\"([!\\\"#\\$%\\'\\(\\)\\*\\+,\\.:;\\-<=·>?@\\[\\\\\\]\\^_ـ`{\\|}~—٪’،؟`୍“؛”ۚ【»؛\\s+«–…‘]{2,})\"\n",
        ")\n",
        "\n",
        "REGEX_TATWEEL = r\"(\\D)\\1{2,}\"\n",
        "MULTIPLE_CHAR_PATTERN = re.compile(r\"(\\D)\\1{2,}\", re.DOTALL)\n",
        "\n",
        "REJECTED_CHARS_REGEX = r\"[^0-9\\u0621-\\u063A\\u0640-\\u066C\\u0671-\\u0674a-zA-Z\\[\\]!\\\"#\\$%\\'\\(\\)\\*\\+,\\.:;\\-<=·>?@\\[\\\\\\]\\^_ـ`{\\|}~—٪’،؟`୍“؛”ۚ»؛\\s+«–…‘]\"\n",
        "REJECTED_CHARS_REGEXV2 = r\"[^0-9\\u0621-\\u063A\\u0641-\\u066C\\u0671-\\u0674a-zA-Z\\[\\]!\\\"#\\$%\\'\\(\\)\\*\\+,\\.:;\\-<=·>?@\\[\\\\\\]\\^_ـ`{\\|}~—٪’،؟`୍“؛”ۚ»؛\\s+«–…‘/]\"\n",
        "\n",
        "REGEX_URL_STEP1 = r\"(?=http)[^\\s]+\"\n",
        "REGEX_URL_STEP2 = r\"(?=www)[^\\s]+\"\n",
        "REGEX_URL = r\"(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
        "REGEX_MENTION = r\"@[\\w\\d]+\"\n",
        "REGEX_EMAIL = r\"\\S+@\\S+\"\n",
        "\n",
        "CHARS_REGEX = r\"0-9\\u0621-\\u063A\\u0640-\\u066C\\u0671-\\u0674a-zA-Z\\[\\]!\\\"#\\$%\\'\\(\\)\\*\\+,\\.:;\\-<=·>?@\\[\\\\\\]\\^_ـ`{\\|}~—٪’،؟`୍“؛”ۚ»؛\\s+«–…‘\"\n",
        "CHARS_REGEXV2 = r\"0-9\\u0621-\\u063A\\u0640-\\u066C\\u0671-\\u0674a-zA-Z\\[\\]!\\\"#\\$%\\'\\(\\)\\*\\+,\\.:;\\-<=·>?@\\[\\\\\\]\\^_ـ`{\\|}~—٪’،؟`୍“؛”ۚ»؛\\s+«–…‘/\"\n",
        "\n",
        "WHITE_SPACED_DOUBLE_QUOTATION_REGEX = r'\\\"\\s+([^\"]+)\\s+\\\"'\n",
        "WHITE_SPACED_SINGLE_QUOTATION_REGEX = r\"\\'\\s+([^']+)\\s+\\'\"\n",
        "WHITE_SPACED_BACK_QUOTATION_REGEX = r\"\\`\\s+([^`]+)\\s+\\`\"\n",
        "WHITE_SPACED_EM_DASH = r\"\\—\\s+([^—]+)\\s+\\—\"\n",
        "\n",
        "LEFT_SPACED_CHARS = r\" ([\\]!#\\$%\\),\\.:;\\?}٪’،؟”؛…»·])\"\n",
        "RIGHT_SPACED_CHARS = r\"([\\[\\(\\{“«‘*\\~]) \"\n",
        "LEFT_AND_RIGHT_SPACED_CHARS = r\" ([\\+\\-\\<\\=\\>\\@\\\\\\^\\_\\|\\–]) \"\n",
        "\n",
        "_HINDI_NUMS = \"٠١٢٣٤٥٦٧٨٩\"\n",
        "_ARABIC_NUMS = \"0123456789\"\n",
        "HINDI_TO_ARABIC_MAP = str.maketrans(_HINDI_NUMS, _ARABIC_NUMS)\n",
        "\n",
        "\n",
        "class ArabertPreprocessor:\n",
        "    \"\"\"\n",
        "    A Preprocessor class that cleans and preprocesses text for all models in the AraBERT repo.\n",
        "    It also can unprocess the text ouput of the generated text\n",
        "    Args:\n",
        "        model_name (:obj:`str`): model name from the HuggingFace Models page without\n",
        "        the aubmindlab tag. Will default to a base Arabic preprocessor if model name was not found.\n",
        "        Current accepted models are:\n",
        "            - \"bert-base-arabertv01\"\n",
        "            - \"bert-base-arabert\"\n",
        "            - \"bert-base-arabertv02\"\n",
        "            - \"bert-base-arabertv2\"\n",
        "            - \"bert-base-arabertv02-twitter\"\n",
        "            - \"bert-large-arabertv02\"\n",
        "            - \"bert-large-arabertv2\"\n",
        "            - \"bert-large-arabertv02-twitter\"\n",
        "            - \"araelectra-base\"\n",
        "            - \"araelectra-base-discriminator\"\n",
        "            - \"araelectra-base-generator\"\n",
        "            - \"araelectra-base-artydiqa\"\n",
        "            - \"aragpt2-base\"\n",
        "            - \"aragpt2-medium\"\n",
        "            - \"aragpt2-large\"\n",
        "            - \"aragpt2-mega\"\n",
        "        remove_html_markup(:obj: `bool`, `optional`, defaults to :obj:`True`): Whether to remove html artfacts,\n",
        "        should be set to False when preprocessing TyDi QA.\n",
        "        replace_urls_emails_mentions(:obj:`bool`, `optional`, defaults to :obj:`True`): Whether to replace email urls\n",
        "        and mentions by special tokens.\n",
        "        strip_tashkeel(:obj:`bool`, `optional`, defaults to :obj:`True`): remove diacritics (FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA,\n",
        "        KASRA, SUKUN, SHADDA).\n",
        "        strip_tatweel(:obj:`bool`, `optional`, defaults to :obj:`True`): remove tatweel '\\\\u0640'.\n",
        "        insert_white_spaces(:obj:`bool`, `optional`, defaults to :obj:`True`): insert whitespace before and after all non Arabic digits\n",
        "        or English digits or Arabic and English Alphabet or the 2 brackets, then inserts whitespace\n",
        "        between words and numbers or numbers and words.\n",
        "        remove_non_digit_repetition(:obj:`bool`, `optional`, defaults to :obj:`True`): replace repetition of more than 2 non-digit character with\n",
        "        2 of this character.\n",
        "        replace_slash_with_dash(:obj:`bool`, `optional`, defaults to :obj:`None`): Will be automatically set to True in AraBERTv02,\n",
        "        AraELECTRA and AraGPT2.\n",
        "        Set to False to force disable, and True to force enable. Replaces the \"/\"  with \"-\",\n",
        "        since \"/\" is missing from AraBERTv2, AraELECTRA and ARAGPT2 vocabulary.\n",
        "        map_hindi_numbers_to_arabic(:obj:`bool`, `optional`, defaults to :obj:`None`): Will be automatically set to True in\n",
        "        AraBERTv02, AraELECTRA and AraGPT2.Set to False to force disable, and True to force enable.\n",
        "        Replaces hindi numbers with the corresponding Arabic one. ex: \"١٩٩٥\" --> \"1995\".\n",
        "        This is behavior is present by default in AraBERTv1 and v2 (with pre-segmentation),\n",
        "        and fixes the issue of caused by a bug when inserting white spaces.\n",
        "        apply_farasa_segmentation(:obj:`bool`, `optional`, defaults to :obj:`None`): Will be automatically set to True in\n",
        "        AraBERTv2, and AraBERTv1. Set to False to force disable, and True to force enable.\n",
        "        keep_emojis(:obj:`bool`, `optional`, defaults to :obj:`None`): don't remove emojis while preprocessing.\n",
        "        Will be automatically set to True in AraBERT trained on tweets.\n",
        "    Returns:\n",
        "        ArabertPreprocessor: A preprocessor instance\n",
        "    Example:\n",
        "        from preprocess import ArabertPreprocessor\n",
        "        arabert_prep = ArabertPreprocessor(\"aubmindlab/bert-base-arabertv2\")\n",
        "        arabert_prep.preprocess(\"SOME ARABIC TEXT\")\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        remove_html_markup: bool = True,\n",
        "        replace_urls_emails_mentions: bool = True,\n",
        "        strip_tashkeel: bool = True,\n",
        "        strip_tatweel: bool = True,\n",
        "        insert_white_spaces: bool = True,\n",
        "        remove_non_digit_repetition: bool = True,\n",
        "        keep_emojis: bool = None,\n",
        "        replace_slash_with_dash: bool = None,\n",
        "        map_hindi_numbers_to_arabic: bool = None,\n",
        "        apply_farasa_segmentation: bool = None,\n",
        "    ):\n",
        "\n",
        "        model_name = model_name.replace(\"aubmindlab/\", \"\").replace(\"wissamantoun/\", \"\")\n",
        "\n",
        "        if model_name not in ACCEPTED_MODELS:\n",
        "            logging.warning(\n",
        "                \"\"\"Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\"\"\"\n",
        "            )\n",
        "            self.model_name = \"bert-base-arabertv02\"\n",
        "        else:\n",
        "            self.model_name = model_name\n",
        "\n",
        "        if apply_farasa_segmentation is None:\n",
        "            if self.model_name in SEGMENTED_MODELS:\n",
        "                self.apply_farasa_segmentation = True\n",
        "            else:\n",
        "                self.apply_farasa_segmentation = False\n",
        "        else:\n",
        "            if (\n",
        "                apply_farasa_segmentation == False\n",
        "                and self.model_name in SEGMENTED_MODELS\n",
        "            ):\n",
        "                logging.warning(\n",
        "                    \"The selected model_name requires Farasa pre-segmentation, but apply_farasa_segmentation was set to False!\"\n",
        "                )\n",
        "\n",
        "            self.apply_farasa_segmentation = apply_farasa_segmentation\n",
        "\n",
        "        if self.apply_farasa_segmentation:\n",
        "            try:\n",
        "                from farasa.segmenter import FarasaSegmenter\n",
        "\n",
        "                self.farasa_segmenter = FarasaSegmenter(interactive=True)\n",
        "            except ModuleNotFoundError:\n",
        "                logging.error(\n",
        "                    \"farasapy is not installed, you want be able to process text for AraBERTv1 and v2. Install it using: pip install farasapy\"\n",
        "                )\n",
        "\n",
        "        if keep_emojis is None:\n",
        "            if self.model_name in TWEET_MODELS:\n",
        "                self.keep_emojis = True\n",
        "            else:\n",
        "                self.keep_emojis = False\n",
        "        else:\n",
        "            if keep_emojis == False and self.model_name in TWEET_MODELS:\n",
        "                logging.warning(\n",
        "                    \"The selected model_name is trained on emojis, but keep_emojis was set to False!\"\n",
        "                )\n",
        "            self.keep_emojis = keep_emojis\n",
        "\n",
        "        if self.keep_emojis:\n",
        "            import emoji\n",
        "\n",
        "            self.emoji = emoji\n",
        "            if self.apply_farasa_segmentation:\n",
        "                logging.warning(\n",
        "                    \"Keeping tweets with Farasa Segmentation is 10 times slower\"\n",
        "                )\n",
        "            emoji_regex = \"\".join(list(self.emoji.UNICODE_EMOJI[\"en\"].keys()))\n",
        "            self.REJECTED_CHARS_REGEX = \"[^%s%s]\" % (\n",
        "                CHARS_REGEX if self.model_name in SECOND_GEN_MODELS else CHARS_REGEXV2,\n",
        "                emoji_regex,\n",
        "            )\n",
        "        else:\n",
        "            self.REJECTED_CHARS_REGEX = (\n",
        "                REJECTED_CHARS_REGEX\n",
        "                if self.model_name in SECOND_GEN_MODELS\n",
        "                else REJECTED_CHARS_REGEXV2\n",
        "            )\n",
        "\n",
        "        self.remove_html_markup = remove_html_markup\n",
        "        self.replace_urls_emails_mentions = replace_urls_emails_mentions\n",
        "        self.strip_tashkeel = strip_tashkeel\n",
        "        self.strip_tatweel = strip_tatweel\n",
        "        self.insert_white_spaces = insert_white_spaces\n",
        "        self.remove_non_digit_repetition = remove_non_digit_repetition\n",
        "\n",
        "        if replace_slash_with_dash is None:\n",
        "            if self.model_name in SECOND_GEN_MODELS:\n",
        "                self.replace_slash_with_dash = True\n",
        "            else:\n",
        "                self.replace_slash_with_dash = False\n",
        "        else:\n",
        "            self.replace_slash_with_dash = replace_slash_with_dash\n",
        "\n",
        "        if map_hindi_numbers_to_arabic is None:\n",
        "            if self.model_name in SECOND_GEN_MODELS:\n",
        "                self.map_hindi_numbers_to_arabic = True\n",
        "            else:\n",
        "                self.map_hindi_numbers_to_arabic = False\n",
        "        else:\n",
        "            self.map_hindi_numbers_to_arabic = map_hindi_numbers_to_arabic\n",
        "\n",
        "    def preprocess(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Preprocess takes an input text line an applies the same preprocessing used in AraBERT\n",
        "                            pretraining, or according to settings\n",
        "        Args:\n",
        "            text (:obj:`str`): inout text string\n",
        "        Returns:\n",
        "            string: A preprocessed string depending on which model was selected\n",
        "        \"\"\"\n",
        "        if (\n",
        "            self.model_name == \"bert-base-arabert\"\n",
        "            or self.model_name == \"bert-base-arabertv01\"\n",
        "        ):\n",
        "            return self._preprocess_v1(\n",
        "                text,\n",
        "                do_farasa_tokenization=self.apply_farasa_segmentation,\n",
        "            )\n",
        "\n",
        "        if self.model_name in SECOND_GEN_MODELS:\n",
        "            return self._preprocess_v2(text)\n",
        "\n",
        "        return self._preprocess_v3(text)\n",
        "\n",
        "    def unpreprocess(self, text: str, desegment: bool = True) -> str:\n",
        "        \"\"\"Re-formats the text to a classic format where punctuations, brackets, parenthesis are not seperated by whitespaces.\n",
        "        The objective is to make the generated text of any model appear natural and not preprocessed.\n",
        "        Args:\n",
        "            text (:obj:`str`): input text to be un-preprocessed\n",
        "            desegment (:obj:`bool`, optional): [whether or not to remove farasa pre-segmentation before]..\n",
        "        Returns:\n",
        "            str: The unpreprocessed (and possibly Farasa-desegmented) text.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.apply_farasa_segmentation and desegment:\n",
        "            text = self.desegment(text)\n",
        "\n",
        "        # removes the spaces around quotation marks ex: i \" ate \" an apple --> i \"ate\" an apple\n",
        "        # https://stackoverflow.com/a/53436792/5381220\n",
        "        text = re.sub(WHITE_SPACED_DOUBLE_QUOTATION_REGEX, '\"' + r\"\\1\" + '\"', text)\n",
        "        text = re.sub(WHITE_SPACED_SINGLE_QUOTATION_REGEX, \"'\" + r\"\\1\" + \"'\", text)\n",
        "        text = re.sub(WHITE_SPACED_BACK_QUOTATION_REGEX, \"\\`\" + r\"\\1\" + \"\\`\", text)\n",
        "        text = re.sub(WHITE_SPACED_EM_DASH, \"\\—\" + r\"\\1\" + \"\\—\", text)\n",
        "\n",
        "        # during generation, sometimes the models don't put a space after the dot, this handles it\n",
        "        text = text.replace(\".\", \" . \")\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        # handle decimals\n",
        "        text = re.sub(r\"(\\d+) \\. (\\d+)\", r\"\\1.\\2\", text)\n",
        "        text = re.sub(r\"(\\d+) \\, (\\d+)\", r\"\\1,\\2\", text)\n",
        "\n",
        "        text = re.sub(LEFT_AND_RIGHT_SPACED_CHARS, r\"\\1\", text)\n",
        "        text = re.sub(LEFT_SPACED_CHARS, r\"\\1\", text)\n",
        "        text = re.sub(RIGHT_SPACED_CHARS, r\"\\1\", text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def desegment(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Use this function if sentence tokenization was done using\n",
        "        `from arabert.preprocess_arabert import preprocess` with Farasa enabled\n",
        "        AraBERT segmentation using Farasa adds a space after the '+' for prefixes,\n",
        "        and after before the '+' for suffixes\n",
        "        Example:\n",
        "        >>> desegment('ال+ دراس +ات')\n",
        "        الدراسات\n",
        "        \"\"\"\n",
        "        text = text.replace(\"+ \", \"+\")\n",
        "        text = text.replace(\" +\", \"+\")\n",
        "        text = \" \".join([self._desegmentword(word) for word in text.split(\" \")])\n",
        "        return text\n",
        "\n",
        "    def _desegmentword(self, orig_word: str) -> str:\n",
        "        \"\"\"\n",
        "        Word segmentor that takes a Farasa Segmented Word and removes the '+' signs\n",
        "        Example:\n",
        "        >>> _desegmentword(\"ال+يومي+ة\")\n",
        "        اليومية\n",
        "        \"\"\"\n",
        "        word = orig_word.replace(\"ل+ال+\", \"لل\")\n",
        "        if \"ال+ال\" not in orig_word:\n",
        "            word = word.replace(\"ل+ال\", \"لل\")\n",
        "        word = word.replace(\"+\", \"\")\n",
        "        word = word.replace(\"للل\", \"لل\")\n",
        "        return word\n",
        "\n",
        "    def _preprocess_v3(self, text: str) -> str:\n",
        "        text = str(text)\n",
        "        text = html.unescape(text)\n",
        "        if self.strip_tashkeel:\n",
        "            text = araby.strip_tashkeel(text)\n",
        "        if self.strip_tatweel:\n",
        "            text = araby.strip_tatweel(text)\n",
        "\n",
        "        if self.replace_urls_emails_mentions:\n",
        "            # replace all possible URLs\n",
        "            for reg in URL_REGEXES:\n",
        "                text = re.sub(reg, \" [رابط] \", text)\n",
        "            # REplace Emails with [بريد]\n",
        "            for reg in EMAIL_REGEXES:\n",
        "                text = re.sub(reg, \" [بريد] \", text)\n",
        "            # replace mentions with [مستخدم]\n",
        "            text = re.sub(USER_MENTION_REGEX, \" [مستخدم] \", text)\n",
        "\n",
        "        if self.remove_html_markup:\n",
        "            # remove html line breaks\n",
        "            text = re.sub(\"<br />\", \" \", text)\n",
        "            # remove html markup\n",
        "            text = re.sub(\"</?[^>]+>\", \" \", text)\n",
        "\n",
        "        if self.map_hindi_numbers_to_arabic:\n",
        "            text = text.translate(HINDI_TO_ARABIC_MAP)\n",
        "\n",
        "        # remove repeated characters >2\n",
        "        if self.remove_non_digit_repetition:\n",
        "            text = self._remove_non_digit_repetition(text)\n",
        "\n",
        "        # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets\n",
        "        if self.insert_white_spaces:\n",
        "            text = re.sub(\n",
        "                \"([^0-9\\u0621-\\u063A\\u0641-\\u064A\\u0660-\\u0669a-zA-Z ])\",\n",
        "                r\" \\1 \",\n",
        "                text,\n",
        "            )\n",
        "\n",
        "            # re-fix brackets\n",
        "            text = text.replace(\"[ رابط ]\", \"[رابط]\")\n",
        "            text = text.replace(\"[ بريد ]\", \"[بريد]\")\n",
        "            text = text.replace(\"[ مستخدم ]\", \"[مستخدم]\")\n",
        "\n",
        "            # insert whitespace between words and numbers or numbers and words\n",
        "            text = re.sub(\n",
        "                \"(\\d+)([\\u0621-\\u063A\\u0641-\\u064A\\u066A-\\u066C\\u0654-\\u0655]+)\",\n",
        "                r\" \\1 \\2 \",\n",
        "                text,\n",
        "            )\n",
        "            text = re.sub(\n",
        "                \"([\\u0621-\\u063A\\u0641-\\u064A\\u066A-\\u066C\\u0654-\\u0655]+)(\\d+)\",\n",
        "                r\" \\1 \\2 \",\n",
        "                text,\n",
        "            )\n",
        "\n",
        "        # remove unwanted characters\n",
        "        text = re.sub(self.REJECTED_CHARS_REGEX, \" \", text)\n",
        "\n",
        "        # remove extra spaces\n",
        "        text = \" \".join(text.replace(\"\\uFE0F\", \"\").split())\n",
        "\n",
        "        if self.apply_farasa_segmentation:\n",
        "            if self.keep_emojis:\n",
        "                new_text = []\n",
        "                for word in text.split():\n",
        "                    if word in list(self.emoji.UNICODE_EMOJI[\"en\"].keys()):\n",
        "                        new_text.append(word)\n",
        "                    else:\n",
        "                        new_text.append(self.farasa_segmenter.segment(word))\n",
        "                text = \" \".join(new_text)\n",
        "            else:\n",
        "                text = self.farasa_segmenter.segment(text)\n",
        "            return self._farasa_segment(text)\n",
        "\n",
        "        # ALl the other models dont require Farasa Segmentation\n",
        "        return text\n",
        "\n",
        "    def _preprocess_v2(self, text: str) -> str:\n",
        "        text = str(text)\n",
        "        text = html.unescape(text)\n",
        "        if self.strip_tashkeel:\n",
        "            text = araby.strip_tashkeel(text)\n",
        "        if self.strip_tatweel:\n",
        "            text = araby.strip_tatweel(text)\n",
        "\n",
        "        if self.replace_urls_emails_mentions:\n",
        "            # replace all possible URLs\n",
        "            for reg in URL_REGEXES:\n",
        "                text = re.sub(reg, \" [رابط] \", text)\n",
        "            # REplace Emails with [بريد]\n",
        "            for reg in EMAIL_REGEXES:\n",
        "                text = re.sub(reg, \" [بريد] \", text)\n",
        "            # replace mentions with [مستخدم]\n",
        "            text = re.sub(USER_MENTION_REGEX, \" [مستخدم] \", text)\n",
        "\n",
        "        if self.remove_html_markup:\n",
        "            # remove html line breaks\n",
        "            text = re.sub(\"<br />\", \" \", text)\n",
        "            # remove html markup\n",
        "            text = re.sub(\"</?[^>]+>\", \" \", text)\n",
        "\n",
        "        if self.map_hindi_numbers_to_arabic:\n",
        "            text = text.translate(HINDI_TO_ARABIC_MAP)\n",
        "\n",
        "        # remove repeated characters >2\n",
        "        if self.remove_non_digit_repetition:\n",
        "            text = self._remove_non_digit_repetition(text)\n",
        "\n",
        "        # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets\n",
        "        if self.insert_white_spaces:\n",
        "            text = re.sub(\n",
        "                \"([^0-9\\u0621-\\u063A\\u0641-\\u064A\\u0660-\\u0669a-zA-Z\\[\\]])\",\n",
        "                r\" \\1 \",\n",
        "                text,\n",
        "            )\n",
        "\n",
        "            # insert whitespace between words and numbers or numbers and words\n",
        "            text = re.sub(\n",
        "                \"(\\d+)([\\u0621-\\u063A\\u0641-\\u064A\\u0660-\\u066C]+)\", r\" \\1 \\2 \", text\n",
        "            )\n",
        "            text = re.sub(\n",
        "                \"([\\u0621-\\u063A\\u0641-\\u064A\\u0660-\\u066C]+)(\\d+)\", r\" \\1 \\2 \", text\n",
        "            )\n",
        "\n",
        "        if self.replace_slash_with_dash:\n",
        "            text = text.replace(\"/\", \"-\")\n",
        "\n",
        "        # remove unwanted characters\n",
        "        text = re.sub(self.REJECTED_CHARS_REGEX, \" \", text)\n",
        "\n",
        "        # remove extra spaces\n",
        "        text = \" \".join(text.replace(\"\\uFE0F\", \"\").split())\n",
        "\n",
        "        if (\n",
        "            self.model_name == \"bert-base-arabertv2\"\n",
        "            or self.model_name == \"bert-large-arabertv2\"\n",
        "        ):\n",
        "            if self.keep_emojis:\n",
        "                new_text = []\n",
        "                for word in text.split():\n",
        "                    if word in list(self.emoji.UNICODE_EMOJI[\"en\"].keys()):\n",
        "                        new_text.append(word)\n",
        "                    else:\n",
        "                        new_text.append(self.farasa_segmenter.segment(word))\n",
        "                text = \" \".join(new_text)\n",
        "            else:\n",
        "                text = self.farasa_segmenter.segment(text)\n",
        "            return self._farasa_segment(text)\n",
        "\n",
        "        # ALl the other models dont require Farasa Segmentation\n",
        "        return text\n",
        "\n",
        "    def _preprocess_v1(self, text: str, do_farasa_tokenization: bool) -> str:\n",
        "        \"\"\"\n",
        "        AraBERTv1 preprocessing Function\n",
        "        \"\"\"\n",
        "        text = str(text)\n",
        "        if self.strip_tashkeel:\n",
        "            text = araby.strip_tashkeel(text)\n",
        "\n",
        "        text = re.sub(r\"\\d+\\/[ء-ي]+\\/\\d+\\]\", \"\", text)\n",
        "        text = re.sub(\"ـ\", \"\", text)\n",
        "        text = re.sub(\"[«»]\", ' \" ', text)\n",
        "\n",
        "        if self.replace_urls_emails_mentions:\n",
        "            # replace the [رابط] token with space if you want to clean links\n",
        "            text = re.sub(REGEX_URL_STEP1, \"[رابط]\", text)\n",
        "            text = re.sub(REGEX_URL_STEP2, \"[رابط]\", text)\n",
        "            text = re.sub(REGEX_URL, \"[رابط]\", text)\n",
        "            text = re.sub(REGEX_EMAIL, \"[بريد]\", text)\n",
        "            text = re.sub(REGEX_MENTION, \"[مستخدم]\", text)\n",
        "        text = re.sub(\"…\", r\"\\.\", text).strip()\n",
        "        text = self._remove_redundant_punct(text)\n",
        "\n",
        "        if self.replace_urls_emails_mentions:\n",
        "            text = re.sub(r\"\\[ رابط \\]|\\[ رابط\\]|\\[رابط \\]\", \" [رابط] \", text)\n",
        "            text = re.sub(r\"\\[ بريد \\]|\\[ بريد\\]|\\[بريد \\]\", \" [بريد] \", text)\n",
        "            text = re.sub(r\"\\[ مستخدم \\]|\\[ مستخدم\\]|\\[مستخدم \\]\", \" [مستخدم] \", text)\n",
        "\n",
        "        if self.remove_non_digit_repetition:\n",
        "            text = self._remove_non_digit_repetition(text)\n",
        "\n",
        "        if self.insert_white_spaces:\n",
        "            text = re.sub(\n",
        "                \"([^0-9\\u0621-\\u063A\\u0641-\\u0669\\u0671-\\u0673a-zA-Z\\[\\]])\",\n",
        "                r\" \\1 \",\n",
        "                text,\n",
        "            )\n",
        "        if do_farasa_tokenization:\n",
        "            text = self._tokenize_arabic_words_farasa(text)\n",
        "\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _farasa_segment(self, text: str) -> str:\n",
        "        line_farasa = text.split()\n",
        "        segmented_line = []\n",
        "        for index, word in enumerate(line_farasa):\n",
        "            if word in [\"[\", \"]\"]:\n",
        "                continue\n",
        "            if word in [\"رابط\", \"بريد\", \"مستخدم\"] and line_farasa[index - 1] in [\n",
        "                \"[\",\n",
        "                \"]\",\n",
        "            ]:\n",
        "                segmented_line.append(\"[\" + word + \"]\")\n",
        "                continue\n",
        "            if \"+\" not in word:\n",
        "                segmented_line.append(word)\n",
        "                continue\n",
        "            segmented_word = self._split_farasa_output(word)\n",
        "            segmented_line.extend(segmented_word)\n",
        "\n",
        "        return \" \".join(segmented_line)\n",
        "\n",
        "    def _split_farasa_output(self, word: str) -> str:\n",
        "        segmented_word = []\n",
        "        temp_token = \"\"\n",
        "        for i, c in enumerate(word):\n",
        "            if c == \"+\":\n",
        "                # if the token is KAF, it could be a suffix or prefix\n",
        "                if temp_token == \"ك\":\n",
        "                    # if we are at the second token, then KAF is surely a prefix\n",
        "                    if i == 1:\n",
        "                        segmented_word.append(temp_token + \"+\")\n",
        "                        temp_token = \"\"\n",
        "                    # If the KAF token is between 2 tokens\n",
        "                    elif word[i - 2] == \"+\":\n",
        "                        # if the previous token is prefix, then this KAF must be a prefix\n",
        "                        if segmented_word[-1][-1] == \"+\":\n",
        "                            segmented_word.append(temp_token + \"+\")\n",
        "                            temp_token = \"\"\n",
        "                        # else it is a suffix, this KAF could not be a second suffix\n",
        "                        else:\n",
        "                            segmented_word.append(\"+\" + temp_token)\n",
        "                            temp_token = \"\"\n",
        "                    # if Kaf is at the end, this is handled with the statement after the loop\n",
        "                elif temp_token in PREFIX_LIST:\n",
        "                    segmented_word.append(temp_token + \"+\")\n",
        "                    temp_token = \"\"\n",
        "                elif temp_token in SUFFIX_LIST:\n",
        "                    segmented_word.append(\"+\" + temp_token)\n",
        "                    temp_token = \"\"\n",
        "                else:\n",
        "                    segmented_word.append(temp_token)\n",
        "                    temp_token = \"\"\n",
        "                continue\n",
        "            temp_token += c\n",
        "        if temp_token != \"\":\n",
        "            if temp_token in SUFFIX_LIST:\n",
        "                segmented_word.append(\"+\" + temp_token)\n",
        "            else:\n",
        "                segmented_word.append(temp_token)\n",
        "        return segmented_word\n",
        "\n",
        "    def _tokenize_arabic_words_farasa(self, line_input: str) -> str:\n",
        "\n",
        "        if self.keep_emojis:\n",
        "            # insert whitespace before and after all non Arabic digits or English Digits and Alphabet and the 2 brackets\n",
        "            line_farasa = []\n",
        "            for word in line_input.split():\n",
        "                if word in list(self.emoji.UNICODE_EMOJI[\"en\"].keys()):\n",
        "                    line_farasa.append(word)\n",
        "                else:\n",
        "                    line_farasa.append(self.farasa_segmenter.segment(word))\n",
        "        else:\n",
        "            line_farasa = self.farasa_segmenter.segment(line_input).split()\n",
        "\n",
        "        segmented_line = []\n",
        "        for index, word in enumerate(line_farasa):\n",
        "            if word in [\"[\", \"]\"]:\n",
        "                continue\n",
        "            if word in [\"رابط\", \"بريد\", \"مستخدم\"] and line_farasa[index - 1] in [\n",
        "                \"[\",\n",
        "                \"]\",\n",
        "            ]:\n",
        "                segmented_line.append(\"[\" + word + \"]\")\n",
        "                continue\n",
        "            segmented_word = []\n",
        "            for token in word.split(\"+\"):\n",
        "                if token in PREFIX_LIST:\n",
        "                    segmented_word.append(token + \"+\")\n",
        "                elif token in SUFFIX_LIST:\n",
        "                    segmented_word.append(\"+\" + token)\n",
        "                else:\n",
        "                    segmented_word.append(token)\n",
        "            segmented_line.extend(segmented_word)\n",
        "        return \" \".join(segmented_line)\n",
        "\n",
        "    def _remove_non_digit_repetition(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        :param text:  the input text to remove elongation\n",
        "        :return: delongated text\n",
        "        \"\"\"\n",
        "        # loop over the number of times the regex matched the text\n",
        "        # OLD\n",
        "        # for index_ in range(len(re.findall(REGEX_TATWEEL, text))):\n",
        "        #     elongation = re.search(REGEX_TATWEEL, text)\n",
        "        #     if elongation:\n",
        "        #         elongation_pattern = elongation.group()\n",
        "        #         elongation_replacement = elongation_pattern[0]\n",
        "        #         elongation_pattern = re.escape(elongation_pattern)\n",
        "        #         text = re.sub(\n",
        "        #             elongation_pattern, elongation_replacement, text, flags=re.MULTILINE\n",
        "        #         )\n",
        "        #     else:\n",
        "        #         break\n",
        "\n",
        "        # New\n",
        "        text = MULTIPLE_CHAR_PATTERN.sub(r\"\\1\\1\", text)\n",
        "        return text\n",
        "\n",
        "    def _remove_redundant_punct(self, text: str) -> str:\n",
        "        text_ = text\n",
        "        result = re.search(REDUNDANT_PUNCT_PATTERN, text)\n",
        "        dif = 0\n",
        "        while result:\n",
        "            sub = result.group()\n",
        "            sub = sorted(set(sub), key=sub.index)\n",
        "            sub = \" \" + \"\".join(list(sub)) + \" \"\n",
        "            text = \"\".join(\n",
        "                (text[: result.span()[0] + dif], sub, text[result.span()[1] + dif :])\n",
        "            )\n",
        "            text_ = \"\".join(\n",
        "                (text_[: result.span()[0]], text_[result.span()[1] :])\n",
        "            ).strip()\n",
        "            dif = abs(len(text) - len(text_))\n",
        "            result = re.search(REDUNDANT_PUNCT_PATTERN, text_)\n",
        "        text = re.sub(r\"\\s+\", \" \", text)\n",
        "        return text.strip()"
      ],
      "metadata": {
        "id": "1eETtSXO2BnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/aub-mind/arabert.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3Enlj424m9L",
        "outputId": "7514b748-8ebf-46c5-d094-794629584a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/aub-mind/arabert.git\n",
            "  Cloning https://github.com/aub-mind/arabert.git to /tmp/pip-req-build-rq__8sn5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/aub-mind/arabert.git /tmp/pip-req-build-rq__8sn5\n",
            "  Resolved https://github.com/aub-mind/arabert.git to commit 6fcebaebc97844d4b498900daa6314257f22c042\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyArabic in /usr/local/lib/python3.8/dist-packages (from arabert==1.0.1) (0.6.15)\n",
            "Requirement already satisfied: farasapy in /usr/local/lib/python3.8/dist-packages (from arabert==1.0.1) (0.0.14)\n",
            "Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.8/dist-packages (from arabert==1.0.1) (1.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farasapy->arabert==1.0.1) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farasapy->arabert==1.0.1) (4.64.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from PyArabic->arabert==1.0.1) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert==1.0.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert==1.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert==1.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy->arabert==1.0.1) (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9L3W-nFQ47KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraForQuestionAnswering, ElectraForSequenceClassification, AutoTokenizer, pipeline\n",
        "#from preprocess import ArabertPreprocessor\n",
        "#import ArabertPreprocessor\n",
        "#prep_object = ArabertPreprocessor(\"araelectra-base-discriminator\")\n",
        "arabert_prep = ArabertPreprocessor(\"aubmindlab/bert-base-arabertv2\")\n",
        "question= arabert_prep.preprocess(\"ما هي جامعة الدول العربية ؟\")\n",
        "#question = prep_object('ما هي جامعة الدول العربية ؟')\n",
        "context = arabert_prep.preprocess('''\n",
        "جامعة الدول العربية هيمنظمة إقليمية تضم دولاً عربية في آسيا وأفريقيا.\n",
        "ينص ميثاقها على التنسيق بين الدول الأعضاء في الشؤون الاقتصادية، ومن ضمنها العلاقات التجارية الاتصالات، العلاقات الثقافية، الجنسيات ووثائق وأذونات السفر والعلاقات الاجتماعية والصحة. المقر الدائم لجامعة الدول العربية يقع في القاهرة، عاصمة مصر (تونس من 1979 إلى 1990).\n",
        "''')\n",
        "# a) Get predictions\n",
        "qa_modelname = 'ZeyadAhmed/AraElectra-Arabic-SQuADv2-QA'\n",
        "cls_modelname = 'ZeyadAhmed/AraElectra-Arabic-SQuADv2-CLS'\n",
        "qa_pipe = pipeline('question-answering', model=qa_modelname, tokenizer=qa_modelname)\n",
        "QA_input = {\n",
        "    'question': question,\n",
        "    'context': context\n",
        "}\n",
        "CLS_input = {\n",
        "    'text': question,\n",
        "    'text_pair': context\n",
        "}\n",
        "qa_res = qa_pipe(QA_input)\n",
        "\n",
        "#removing this line temporarily\n",
        "#cls_res = cls_pipe(CLS_iput)\n",
        "\n",
        "threshold = 0.5 #hyperparameter can be tweaked\n",
        "## note classification results label0 probability it can be answered label1 probability can't be answered\n",
        "## if label1 probability > threshold then consider the output of qa_res is empty string else take the qa_res\n",
        "# b) Load model & tokenizer\n",
        "qa_model = ElectraForQuestionAnswering.from_pretrained(qa_modelname)\n",
        "cls_model = ElectraForSequenceClassification.from_pretrained(cls_modelname)\n",
        "tokenizer = AutoTokenizer.from_pretrained(qa_modelname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsiIzH6m1O8_",
        "outputId": "8faea10d-ccf9-44f4-ef8d-846480c16d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-01-04 10:13:33,874 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install xlsx2csv\n",
        "from datasets import load_dataset\n",
        "xlsx2csv \"/dataset.xlsx\" \"file.csv\"\n",
        "# dataset = load_dataset(\"/dataset.xlsx\")\n",
        "# dataset[\"train\"][100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "je-3hRr5RkRf",
        "outputId": "d4134edf-cb2a-44b5-f3bc-ce3b08f94a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-3bd59095f04f>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    xlsx2csv \"/dataset.xlsx\" \"file.csv\"\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T46nukF4TIH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzRzIqEb1P4D",
        "outputId": "53f8e3f9-4f29-45f6-db09-35e6318a3d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='ZeyadAhmed/AraElectra-Arabic-SQuADv2-QA', vocab_size=64000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLy6cj2f95BR",
        "outputId": "e283502e-3e18-4109-aa5c-6ab98500a607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.02556466870009899,\n",
              " 'start': 68,\n",
              " 'end': 83,\n",
              " 'answer': 'آسيا و+ أفريقيا'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The answer of the above mentioned question"
      ],
      "metadata": {
        "id": "9oC_lhn3kUGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_res[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "VSxPzIaP-BGY",
        "outputId": "53f953fa-f32b-4b72-db06-f9ccf234d4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-03ab4b577ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'qa_res' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjtUXYh6-5oU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}